#This file will filter/censor bad words from the video file

'''
Install packages and dependencies

Initialize the enviornment
The bash script will

1) Download pre-trained Intel Models
2) Create configuration file (needed for making inference on speech)
3) Required pre-requisites for LibriSpeech Model (graph file, etc)
4) Test Online and Offline demos to validate if all pre-requisites were installed properly.
'''
import wave
!apt-get install libsox-fmt-all libsox-dev sox
!pip install ffmpeg
!wget "https://raw.githubusercontent.com/alihussainia/OpenDevLibrary/master/openvino_initialization_script.py"
!python openvino_initialization_script.py
!bash /opt/intel/openvino_2020.1.023/deployment_tools/demo/demo_speech_recognition.sh

#Class
class FilterVideo:
    def __init__(self, video, badList):
        self.video = name
        self.badList = badList
    
    def filter():
        this.runAllCells()



'''
Running Offline DEMO

The output generated by the run_demo.sh is similar to :

[ INFO ] Using feature transformation /root/openvino_models/ir/intel/lspeech_s5_ext/FP32/lspeech_s5_ext.feature_transform
[ INFO ] InferenceEngine API ver. 2.1 (build: 37988)
[ INFO ] Device info:
[ INFO ] CPU: MKLDNNPlugin ver. 2.1
[ INFO ] Batch size: 8
[ INFO ] Model loading time: 49.93 ms
Recognition result:
HOW ARE YOU DOING

We extract this output (in a naive way) by simply asking sed method to filter the console output as we wish to use only the text generated from the speech.

Next, we save this output to a txt file for the next step: forced alignment.

'''
import os
!wget "https://raw.githubusercontent.com/PrashantDandriyal/Speech-Censor-Bot/master/negan.wav"
wav_path = "/content/negan.wav"





'''
Preprocessing audio file
As per the OpenVINO v2020.1 docs here, WAV file needs to be in following format: RIFF WAVE PCM 16bit, 16kHz, 1 channel i.e.,

Sample size : 16bit
Sampling Rate : 16kHz
Number of channels : 1

We preprocess audio and convert it if needed and replace the old file with new.

'''
def preprocess(org_aud_path):
  tx = wave.open(org_aud_path, 'r')
  print ("Initial Parameters:")
  !sox --i "$org_aud_path"
  if(tx.getnchannels() > 1):
    #Convert stereo to mono
    #and replace the original with new
    !sox "$org_aud_path" processed.wav channels 1
    !rm -r "$org_aud_path"
    !mv "processed.wav" "$org_aud_path"
    print("Converted Stereo to Mono")

  if(tx.getframerate() != 16000):
    #Downsample (if > 16k) and Upsample (if < 16k)
    #and replace the original with new
    !sox "$org_aud_path" processed.wav rate 16000
    !rm -r "$org_aud_path"
    !mv "processed.wav" "$org_aud_path"
    print("Changed sample rate to 16k")

    print("Processed file into the same path with name 'processed.wav' ")

preprocess(wav_path)
print("Update file parameters")
!sox --i "$wav_path"











'''
The demo uses the "how_are_you_doing.wav" audio file stored in the location
/opt/intel/openvino/deployment_tools/demo/how_are_you_doing.wav This file is fed to the inference engine 
using the bash file run_demo.sh. Instead of editing another bash file or creating a new one, 
we rename our WAV file to how_are_you_doing.wav` and replace the original file with ours.
'''
%cd "/content/"
!mv "$wav_path" "how_are_you_doing.wav"
!rm -r "/opt/intel/openvino/deployment_tools/demo/how_are_you_doing.wav"
!cp "/content/how_are_you_doing.wav" "/opt/intel/openvino/deployment_tools/demo/"






'''
Perform Inference

The OpenVINO dependencies have successfully been installed and the environment has also been initialized. 
Its time to make the inference ! Run the cell to make inference. As the shell script echoes the result onto the terminal, 
we use sed piping to publish our results onto a text file. 
Another instance of the same command but without this pipe is run, to provide status of the inference.
'''
!/opt/intel/openvino/data_processing/audio/speech_recognition/demos/offline_speech_recognition_demo/run_demo.sh 
#Running again to save the output
!/opt/intel/openvino/data_processing/audio/speech_recognition/demos/offline_speech_recognition_demo/run_demo.sh | sed '1,/Recognition result/d' > /content/out_text.txt





'''
Use gentle to obtain syncmap
Gentle is a forced aligner built on KALDI that automatically generates a synchronization map between a list of text fragments and an audio file containing the narration of the text. 
In computer science this task is known as forced alignment.
'''
json_path = "/content/align.json"







'''
Time to censor

1) Get the list of profanity words from here
2) Detect any such word and mute the uttrance using ffmpeg
'''
%cd /content/
!wget "https://raw.githubusercontent.com/PrashantDandriyal/Google-profanity-words/master/list.txt"
profane_text = "/content/list.txt"

#Make a list out of all the words in the profane_text txt file. 
#This is to avoid repetitive file searching 
with open(profane_text) as f:
  cuss_list = [i.strip() for i in f]
f.close()

#The word "F*Ck" has been recognised as "For", so censoring it for the WAV file "negan.wav" 
cuss_list.append("for")

#Parsing the json content to a easily-accesible dictionary
import json
import pandas as pd

with open(json_path, 'r') as f:
    handle = json.load(f)

df = pd.DataFrame(columns=['word', 'start', 'end'])
rows_list = []

for i in handle['words']:
  #print(i)
  dict1 = {}
  if((i['word']).lower() in cuss_list):
    try:
      dict1.update({"word":i['word'], 
                "start":i['start'],
                "end":i['end']})   
      rows_list.append(dict1) 
    except KeyError:    #Sometimes the word is not properly detected and the entry is "'case': 'not-found-in-audio'"
      pass

df = pd.DataFrame(rows_list)
print(df.head())

#Traverse the entire transcript(text converted from speech) and look for any of such cuss word
#If any such word occurs, save its start and stop time (timestamp) t
test_file = open(text_path,"r")

dff = pd.DataFrame(columns=['swear', 'start', 'end'])
swear_dict_list = []

with open(text_path, 'r') as f:
    for line in f:  
      for word in line.split():
        if(word.upper() in cuss_list):    #Convert the word to Upper case and seach int cuss word list
          d = {}
          d.update({"swear":word, 
                "start":df.loc[word].start,
                "end":df.loc[word].end})
          swear_dict_list.append(d)
dff = pd.DataFrame(swear_dict_list)



wav_path = "/opt/intel/openvino/deployment_tools/demo/how_are_you_doing.wav"
#Format of SOX command to fade in. Refer to(https://stackoverflow.com/questions/20127095/using-sox-to-change-the-volume-level-of-a-range-of-time-in-an-audio-file)
'''
fade [type] fade-in-length [stop-position(=) [fade-out-length]]

sox -m
    -t wav "|sox -V1 inputfile.wav -t wav - fade t 0 2.2 0.4" 
    -t wav "|sox -V1 inputfile.wav -t wav - trim 1.8 fade t 0.4 3.4 0.4 gain -6 pad 1.8"
    -t wav "|sox -V1 inputfile.wav -t wav - trim 4.8 fade t 0.4 0 0 pad 4.8"
    outputfile.wav gain 9.542
'''
for i in df.index: 
  start = df['start'][i]
  end = df['end'][i]
  duration = end-start
  #The duration of transition period when a fades in and fades out
  trans = 0.3
  fade_start = start-(trans/2)
  act_fade_start = start+(trans/2)
  #fade_start -> act_fade_start -> fade_end -> act_fade_end
  fade_end = end-(trans/2)
  act_fade_end = end+(trans/2)

  fade_duration = duration + trans
  print("For ",start,", ", end)
  !sox -m -t wav "|sox -V1 $wav_path -t wav - fade t 0 $act_fade_start $trans" -t wav "|sox -V1 $wav_path -t wav - trim $fade_start fade t $trans $fade_duration $trans gain -40 pad $fade_start" -t wav "|sox -V1 $wav_path -t wav - trim $fade_end fade t $trans 0 0 pad $fade_end" outputfile.wav gain 9.542
  #Replacing the input file with the outputfile
  !rm -r "$wav_path"
  !cp outputfile.wav "$wav_path"

!cp "$wav_path" "/content/final_audio.wav"







'''
        Replace the audio of the video with the censored audio by using ffmpeg
'''

!wget -P "/content/" "https://raw.githubusercontent.com/PrashantDandriyal/Speech-Censor-Bot/master/DocsResources/negan_clip_original.mp4"
vid_path = "/content/negan_clip_original.mp4"

!ffmpeg -i "$vid_path" -i "$wav_path" -c:v copy -map 0:v:0 -map 1:a:0 filter_vid.mp4
#The output is generated in the working directory with the name "filter_vid.mp4"

